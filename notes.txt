EXPERIMENTS

-- CODE2RUN --
source venvs/wild/bin/activate (or conda activate environment -- conda activate wild)
cd <path to wilds>/wilds
export CUDA_VISIBLE_DEVICES=
python examples/run_expt.py --dataset poverty --algorithm ERM --root_dir ./datasets --log_dir ./log/22-11-13-erm-no-center-fold-A --version "1.1" --center_data False --device 0 --dataset_kwargs fold=A

-------------
22-12-30
-------------
Fixed eval, so that normalization *doesn't* occur + use standard data loader
- this fixes issue with trying to create batches with data from the same domain (in this case really should use all
  data form the domain, because otherwise may be left with batches that are too small to accurately estimate
  normalization statistics -- this is the issue I was running into before)

Need to run
- normalization = {z-norm, center}
- lambda = {0, 1} ERM vs IRM
- sweep over FOLDS

time python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-12-30-z-norm-fold-A --version "1.1" --algorithm IRM  --irm_lambda 0 --normalization z --device 0 --gradient_accumulation_steps 8 --batch_size 8  --n_groups_per_batch 1 --dataset_kwargs fold=A
z-norm, LAMBDA 0 (ERM)
- A: running on Milo, tmux 0, gpu 3 real    181m39.053s - DONE
- B: running on Milo, tmux 1, gpu 5 real    181m39.486s - DONE
- C: running on Mars, tmux 1, gpu 1 325m5.297s - DONE
- D: running on Milo, tmux 2, gpu 7 - DONE 183
- E: running on Mars, tmux 2, gpu 2 - DONE 322m20.110s

time python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-12-30-erm-center-fold-A --version "1.1" --algorithm IRM  --irm_lambda 0 --normalization center --device 0 --gradient_accumulation_steps 8 --batch_size 8  --n_groups_per_batch 1 --dataset_kwargs fold=A
center, LAMBDA 0 (ERM)
- A: running on Mars, tmux 3, gpu 3 -- 319m6.543s DONE
- B: running on Twix, tmux 0, gpu 0 -- 317 DONE
- C: running on Twix, tmux 1, gpu 1 -- 316 DONE
- D: running on Twix, tmux 2, gpu 2 -- 314 DONE
- E: running on Twix, tmux 3, gpu 3 -- 315 DONE

time python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-12-30-irm-lmbd-1-z-norm-fold-A --version "1.1" --algorithm IRM  --irm_lambda 1 --normalization z --device 0 --gradient_accumulation_steps 8 --batch_size 8  --n_groups_per_batch 1 --dataset_kwargs fold=A
z-norm, LAMBDA 1
- A: running on Oreo, tmux 0, gpu 1 -- 286 DONE
- B: running on Oreo, tmux 1, gpu 0 -- 282 DONE
- C: running on Milo, tmux 0, gpu 3 -- 192 DONE
- D: running on Milo, tmux 1, gpu 5 -- 191 DONE
- E: running on Milo, tmux 2, gpu 7 --  192 DONE

time python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-12-30-irm-lmbd-1-center-fold-A --version "1.1" --algorithm IRM  --irm_lambda 1 --normalization center --device 0 --gradient_accumulation_steps 8 --batch_size 8  --n_groups_per_batch 1 --dataset_kwargs fold=A
center, LAMBDA 1
- A: running on ahoy, tmux 0, gpu 1 -- 248 DONE
- B: running on ahoy, tmux 1, gpu 2 -- 247 DONE
- C: running on ahoy, tmux 2, gpu 3 -- 242 DONE
- D: running on Milo, tmux 0, gpu 3 -- 183 DONE
- E: running on Milo, tmux 1, gpu 5 -- 184 DONE

-------------
22-12-26
-------------

Run with corrected eval grouper for IRM
- Issue before is that the default data loader for evaluation (val + test) datasets involved using the 'standard'
  group sampler, which just samples each sample in order -- NOT paying attention to groups. So group centers during evaluation
  won't be based on the domain variables.
- I've now fixed this by creating "EvalGroupSampler", which creates batches from each groups data one-by-one (in sorted order)

Experiments that weren't affected by this error: everything but IRM center
So, need to re-run IRM center experiments (and then also run the ERM-centering experiments I had planned).

1. IRM-center EVAL EXP (assuming best lambda is 1)
-- center
-- lambda=1
-- lr default
-- batch size 8
-- n groups per batch 1
-- grad acc steps 8
-- SWEEP over folds

time python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-12-26-center-fold-A --version "1.1" --algorithm IRM  --irm_lambda 1 --center_data True --device 0 --gradient_accumulation_steps 8 --batch_size 8  --n_groups_per_batch 1 --dataset_kwargs fold=A
- A: DONE on Mars, tmux 0, GPU 0 -- 295m41.208s
- B: DONE on Milo, tmux 1, GPU 5 -- 185m27.246s time
- C: DONE on Mars, tmux 2, GPU 2 --  291m42.954s
- D: DONE on Mars, tmux 3, GPU 3 -- 284m52s
- E: DONE on Milo, tmux 0, GPU 3 --  191m10.266s time

2. ERM-center EVAl EXP (w/ parameters chosen based on WILDS v2)
-- center
-- lambda=0
-- lr 0.003671043225227438
-- batch size 15
-- n groups per batch 1
-- grad acc steps 8
-- SWEEP over folds

Note: for this experiment can't run on csail machines -- I get a pin memory error.
But seems to be working on ML 7.

time python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-12-26-erm-center-v2-A --version "1.1" --algorithm IRM  --irm_lambda 0 --center_data True --device 0 --gradient_accumulation_steps 8 --batch_size 15  --n_groups_per_batch 1 --dataset_kwargs fold=A --lr 0.003671043225227438
- A: running Matlaber 7, tmux 1, gpu 0 -- real    1750m1.286s
- B: running on Matlaber 7, tmux 3, gpu 1 -- real    1769m35.152s
- C: running on Matlber 7, tmux 4, gpu 2 -- real    1794m11.257s
- D: running on Matlaber 7, tmux 5, gpu 3 -- real    1798m14.651s
- E: running on Matlaber 7, tmux 6, gpu 0 -- real    1797m2.325s
-- all still runing from last night!

3. ERM-center EVAl EXP (w/ parameters chosen based on WILDS v1)
-- center
-- lambda=0
-- lr default
-- batch size 8
-- n groups per batch 1
-- grad acc steps 8
-- SWEEP over folds

time python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-12-26-erm-center-v1-A --version "1.1" --algorithm IRM  --irm_lambda 0 --center_data True --device 0 --gradient_accumulation_steps 8 --batch_size 8  --n_groups_per_batch 1 --dataset_kwargs fold=A
- A: running on Matlaber 7, tmux 8, gpu 4 -- real    1775m19.741s
- B: running on Matlaber 7, tmux 7, gpu 5 -- real    1774m1.580s
- C: running on Matlaber 7, tmux 9, gpu 6 -- real    1770m43.884s
- D: running on Matlaber 7, tmux 10, gpu 7 -- real    1765m42.555s
- E: running on Milo, tmux 0, gpu 3 -- finished in real 184m29.302s
-- all matlaber runs still going from last night -- that's ridiculous

4. IRM-center PARAMETER SWEEP EXP
Parameter setting to run (IRM runs)
-- center
-- default learning rate
-- fold A
-- batch size 8
-- n groups per batch 1
-- grad acc steps 8
-- SWEEP lambda={10, 100, 1000} (already ran with lambda = 1)

time python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-12-26-irm-center-lambda_10 --version "1.1" --algorithm IRM  --irm_lambda 10 --center_data True --device 0 --gradient_accumulation_steps 8 --batch_size 8  --n_groups_per_batch 1 --dataset_kwargs fold=A

- 10: running on Milo, tmux 1, GPU 5 -- finished in real 182m24.319s
- 100: running on Mars, tmux 2, GPU 2 -- finished in real  272m6.280s
- 1000: running on Mars, tmux 3, GPU 3 -- finished in real 265m11.014s
  Note these jobs failed due to pin memory issue on Mars gpu 0/1



LATER
---+ also look at different batch sizes + more epochs


-------------
22-12-06
-------------
Replicate their ERM results (w/ Wilds V2 parameters)
python examples/run_expt.py --dataset poverty --algorithm ERM --root_dir ./datasets --log_dir ./log/22-12-06-erm-no-center-fold-A --center_data False --device 0 --dataset_kwargs fold=A --seed 0 --batch_size 120 --lr 0.003671043225227438 --loader_kwargs num_workers=4 pin_memory=True
- A [ml 7] [gpu 4] -- tmux 0
- B [ml 7] [gpu 5] -- tmux 1
- C [ml 7] [gpu 6] -- tmux 2
- D [ml 7] [gpu 7] -- tmux 3
- E [ml 4] [gpu 1] -- tmux 0

-------------
22-11-13
-------------
Replicate their ERM results (note their ERM implementation doesn't sample per group)
- A [ml 7] [gpu 0] -- tmux 0
- B [ml 7] [gpu 1] -- tmux 1
- C [ml 7] [gpu 2] -- tmux 2
- D [ml 7] [gpu 3] -- tmux 3
- E [ml 7] [gpu 4] -- tmux 4

./log/22-11-13-erm-no-center-fold-A

-------------
22-11-10
-------------
Run eval of my centering solution with their setup
- A [running] [ml 7] [gpu 2] -- tmux 0
- B [running] [ml 7] [gpu 1] -- tmux 1
- C [running] [ml 7] [gpu 0] -- tmux 2
- D [running] [ml 7] [gpu 3] -- tmux 3
- E [running] [ml 7] [gpu 4] -- tmux 4

python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-11-10-center-fold-A --version "1.1" --algorithm IRM  --irm_lambda 1 --center_data True --device 0 --gradient_accumulation_steps 8 --batch_size 8  --n_groups_per_batch 1 --dataset_kwargs fold=A

-------------
22-11-07
-------------
Actually, what I should really be running for the centering solution to match their setup is
- BS 8, n groups 1, grad acc 8
Running this hyper-parmeter sweep:
- 1 [running] [ml 7] [gpu 5] -- tmux 5
- 10 [running][ml 7] [gpu 6] -- tmux 6
- 100 [running] [ml 7] [gpu 7] -- tmux 7
- 1000 [running] [ml 7] [gpu 2] -- tmux 0

python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-11-07-center-lmbd-1 --version "1.1" --algorithm IRM  --irm_lambda 1 --center_data True --device 0 --gradient_accumulation_steps 8 --batch_size 8  --n_groups_per_batch 1

Examine performance with different batch sizes & number of groups (i.e., grad acc steps)
- BS 128, n groups 1, grad acc 8 (no center); ./log/22-11-07-no-center-lmbd-1-bs-128-ga-8
    python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-11-07-center-lmbd-1-no-center-lmbd-1-bs-128-ga-8 --version "1.1" --algorithm IRM  --irm_lambda 1 --center_data False --device 0 --gradient_accumulation_steps 8 --batch_size 128  --n_groups_per_batch 1
    - 1 [finished] [ml 7] [gpu 0] -- tmux 1
    - 10 [finished] [ml 2] [gpu 2] -- tmux 2
    - 100 [finished] [ml 2] [gpu 3] -- tmux 3
    - 1000 [running] [ml 7] [gpu 5] -- tmux 5

- BS 128, n groups 1, grad acc 8 (center); ./log/22-11-07-center-lmbd-1-bs-128-ga-8
    python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-11-07-center-lmbd-1-center-lmbd-1-bs-128-ga-8 --version "1.1" --algorithm IRM  --irm_lambda 1 --center_data True --device 0 --gradient_accumulation_steps 8 --batch_size 128  --n_groups_per_batch 1
    - 1 [finished] [ml 7] [gpu 1] -- tmux 2
    - 10 [running[ [ml 7] [gpu 6] -- tmux 6
    - 100 [running] [ml 7] [gpu 7] -- tmux 7
    - 1000 [running] [ml 2] [gpu 0] -- tmux 0

- BS 256, n groups 1, grad acc 8 (no center); ./log/22-11-07-center-lmbd-1-no-center-bs-256-ga-8
    python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-11-07-center-lmbd-1-no-center-lmbd-1-bs-256-ga-8 --version "1.1" --algorithm IRM  --irm_lambda 1 --center_data False --device 0 --gradient_accumulation_steps 8 --batch_size 256  --n_groups_per_batch 1
     - 1 [running] [ml 7] [gpu 3] -- tmux 3
     - 10 [running] [ml 9] [gpu 0] -- tmux 0
     - 100 [running] [ml 9] [gpu 1] -- tmux 1
     - 1000 [running] [ml 9] [gpu 2] -- tmux 2

- BS 256, n groups 1, grad acc 8 (center); ./log/22-11-07-center-lmbd-1-center-bs-256-ga-8
    python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-11-07-center-lmbd-1-center-lmbd-1-bs-256-ga-8 --version "1.1" --algorithm IRM  --irm_lambda 1 --center_data True --device 0 --gradient_accumulation_steps 8 --batch_size 256 --n_groups_per_batch 1
    - 1 [running] [ml 7] [gpu 4] -- tmux 4
    - 10 [running] [ml 9] [gpu 3] -- tmux 3
    - 100 [running] [ml 2] [gpu 0] -- tmux 0
    - 1000 [running] [ml 2] [gpu 1] -- tmux 1

-------------
22-11-03
-------------
Run eval - center + their batch size & num-groups -- ACTUALLY KILLED THESE RUNS BECAUSE THEY ARE NOT THE RIGHT SETUP FOR CENTERING
- A
- B
- C
- D
- E

python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-11-03-sb-center-fold-A --version "1.1" --algorithm IRM  --irm_lambda 1 --center_data True --device 0 --batch_size 64  --n_groups_per_batch 8 --dataset_kwargs fold=A

-------------
22-11-01
-------------
Replicate their results (final)
- A [running] [ml 7] [gpu 2] -- tmux 0
- B [running] [ml 7] [gpu 0] -- tmux 1
- C [running] [ml 7] [gpu 1] -- tmux 2
- D [running] [ml 7] [gpu 3] -- tmux 3
- E [running] [ml 7] [gpu 4] -- tmux 4

python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-11-01-repl-fold-A --version "1.1" --algorithm IRM  --irm_lambda 1 --center_data False --device 0 --batch_size 64  --n_groups_per_batch 8 --dataset_kwargs fold=A

Run parameter sweep w/ their batch size & num-groups + centering
- 1 [running] [ml 7] [gpu 5] -- tmux 5
- 10 [running] [ml 7] [gpu 6] -- tmux 6
- 100 [running] [ml 7] [gpu 7] -- tmux 7
- 1000 [running] [ml 7] [gpu 0] -- tmux 1

python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-11-01-sb-center-lmbd-1 --version "1.1" --algorithm IRM  --irm_lambda 1 --center_data True --device 0 --batch_size 64  --n_groups_per_batch 8


--------------
22-10-31
--------------

Replicate their results (parameter sweep)
- 1 [running] [ml 7] [gpu 2] -- tmux 0
- 10 [running] [ml 7] [gpu 0] -- tmux 1
- 100 [running] [ml 7] [gpu 1] -- tmux 2
- 1000 [running] [ml 7] [gpu 3] -- tmux 3

python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-10-31-repl-lmbd-1  --version "1.1" --algorithm IRM  --irm_lambda 1 --center_da
ta False --device 0 --batch_size 64  --n_groups_per_batch 8

--------------
22-10-24
--------------

Center
- 1 [running] [ml 7] [gpu 1] -- tmux 2
- 10 [running ml 7] [gpu 5] -- tmux 5
- 100 [running ml 7] [gpu 6] -- tmux 6
- 1000 [running ml 7] [gpu 7] -- tmux 7

python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-10-24-center-lmbd-1  --version "1.1" --algorithm IRM  --irm_lambda 1 --center_data
True --device 2 --gradient_accumulation_steps 4 --batch_size 256  --n_groups_per_batch 1

No Center
- 1 [running] [ml 7] [gpu 0] -- tmux 1
- 10 [running] [ml 7] [gpu 2] -- tmux 0
- 100 [running ml 7] [gpu 3] -- tmux 3
- 1000 [running ml 7] [gpu 4] -- tmux 4

python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-10-24-no-center-lmbd-1  --version "1.1" --algorithm IRM  --irm_lambda 1 --center_da
ta False --device 0 --gradient_accumulation_steps 4 --batch_size 256  --n_groups_per_batch 1

--------------
22-10-28
--------------

EVAL w/ LMBD=1 EXPERIMENTS

Center
- A [running] [ml 7] [gpu 2] -- tmux 0
- B [running] [ml 7] [gpu 0] -- tmux 1
- C [running] [ml 7] [gpu 1] -- tmux 2
- D [running] [ml 7] [gpu 3] -- tmux 3
- E [running] [ml 7] [gpu 4] -- tmux 4

python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-10-28-center-eval-A  --version "1.1" --algorithm IRM  --irm_lambda 1 --center_data
True --device 0 --gradient_accumulation_steps 4 --batch_size 256  --n_groups_per_batch 1 --dataset_kwargs fold=A

No Center
- A [running] [ml 7] [gpu 5] -- tmux 5
- B [running] [ml 7] [gpu 6] -- tmux 6
- C [running] [ml 7] [gpu 7] -- tmux 7
- D [running] [ml 2] [gpu 0] -- tmux 1
- E [running] [ml 2] [gpu 1] -- tmux 2

python examples/run_expt.py --dataset poverty --root_dir ./datasets --log_dir ./log/22-10-28-no-center-eval-A  --version "1.1" --algorithm IRM  --irm_lambda 1 --center_data
False --device 0 --gradient_accumulation_steps 4 --batch_size 256  --n_groups_per_batch 1 --dataset_kwargs fold=A

